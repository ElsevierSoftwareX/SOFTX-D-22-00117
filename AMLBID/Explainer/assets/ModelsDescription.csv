,id,Name,Ref,Cimport,Cname,Conceptual_desc,details
0,LogisticRegression,Logistic Regression,https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression,Class sklearn.linear_model.,LogisticRegression,"(penalty, dual, tol, C, fit_intercept, intercept_scaling, class_weight, random_state, solver, max_iter, multi_class, verbose, warm_start, n_jobs, l1_ratio)","Logistic regression is a classification algorithm used to find the probability of event success and event failure. Logistic regression is easier to implement, interpret, and very efficient to train. Acheive good accuracy for many simple data sets and it performs well when the dataset is linearly separable. "
1,SVM,Support vector machines,https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC,Class sklearn.svm.,SVC,"(C, kernel, degree, gamma, coef0, shrinking, probability, tol, cache_size, class_weight, verbose, max_iter, decision_function_shape, break_ties, random_state)","Support vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection.The advantages of support vector machines are:
+ Effective in high dimensional spaces.
+ Still effective in cases where number of dimensions is greater than the number of samples.
+ Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.
"
2,AdaBoost,AdaBoost,https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier,Class sklearn.ensemble.,AdaBoostClassifier,"(base_estimator, n_estimators, learning_rate, algorithm, random_state)",AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases.
3,DecisionTree,DecisionTreeClassifier,https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier,Class sklearn.tree.,DecisionTreeClassifier,"(criterion, splitter, max_depth, min_samples_split, min_samples_leaf, min_weight_fraction_leaf, max_features, random_state, max_leaf_nodes, min_impurity_decrease, min_impurity_split, class_weight, ccp_alpha)","Decision tree is on of the most powerful and popular tools for classification and prediction. A Decision tree is a flowchart like tree structure, where each internal node denotes a test on an attribute, each branch represents an outcome of the test, and each leaf node (terminal node) holds a class label. The strengths of decision tree methods are:

+Decision trees are able to generate understandable rules.
+Decision trees perform classification without requiring much computation.
+Decision trees are able to handle both continuous and categorical variables.
+Decision trees provide a clear indication of which fields are most important for prediction or classification."
4,SGDClassifier, SGD Classifier,https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier,Class sklearn.linear_model.SGDClassifier,,"(loss, *, penalty, alpha, l1_ratio, fit_intercept, max_iter, tol, shuffle, verbose, epsilon, n_jobs, random_state, learning_rate, eta0, power_t, early_stopping, validation_fraction, n_iter_no_change, class_weight, warm_start, average=False)","Gradient Descent is a popular optimization technique in Machine Learning and Deep Learning, and it can be used with most, if not all, of the learning algorithms. A gradient is the slope of a function. It measures the degree of change of a variable in response to the changes of another variable."
5,RandomForest,Random Forest Classifier,https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html,Class sklearn.ensemble.,RandomForestClassifier,"(n_estimators, criterion, max_depth, min_samples_split, min_samples_leaf, min_weight_fraction_leaf, max_features, max_leaf_nodes, min_impurity_decrease, min_impurity_split, bootstrap, oob_score, n_jobs, random_state, verbose, warm_start, class_weight, ccp_alpha, max_samples)",Random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.
6,ExtraTrees,Extra Trees Classifier,https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html,Class sklearn.ensemble.,ExtraTreesClassifier,"(n_estimators, criterion, max_depth, min_samples_split, min_samples_leaf, min_weight_fraction_leaf, max_features, max_leaf_nodes, min_impurity_decrease, min_impurity_split, bootstrap, oob_score, n_jobs, random_state, verbose, warm_start, class_weight, ccp_alpha, max_samples)",This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.
7,GradientBoosting,Gradient Boosting Classifier,https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html,Class sklearn.ensemble.,GradientBoostingClassifier,"(loss, learning_rate, n_estimators, subsample, criterion, min_samples_split, min_samples_leaf, min_weight_fraction_leaf, max_depth, min_impurity_decrease, min_impurity_split, init, random_state, max_features, verbose, max_leaf_nodes, warm_start, validation_fraction, n_iter_no_change, tol, ccp_alpha)","Gradient boosting classifiers are a group of machine learning algorithms that combine many weak learning models together to create a strong predictive model . Gradient boosting models are becoming popular because of their effectiveness at classifying complex datasets, and have recently been used to win many Kaggle data science competitions."
